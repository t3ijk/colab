{"cells":[{"cell_type":"markdown","metadata":{"id":"mOtlkKOJk11B"},"source":["##### Copyright 2019 The TensorFlow Authors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2CTPLx3Ukwrv"},"outputs":[],"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"wSIW5dYmdGfE"},"source":["# Transformer Position Encoding\n","\n","<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/community/en/position_encoding.ipynb\">\n","    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n","    Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb\">\n","    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n","    View source on GitHub</a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"ZPoPd7GbdK0G"},"source":["See [this notebook](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb) for a walk-through of full transformer implementation.\n","\n","The transformer architecture uses stacked attention layers in place of CNNs or RNNs. This makes it easy to learn long-range dependencies but it contains no built in information about the relative positions of items in a sequence.\n","\n","To give the model access to this information the transformer architecture adds a position encoding to the input.\n","\n","This encoding is a vector of sines and cosines at each position, where each sine-cosine pair rotates at a different frequency.  \n","\n","Nearby locations will have similar position-encoding vectors."]},{"cell_type":"markdown","metadata":{"id":"XLC5-V6sefM6"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y6BDOVCnKPSW"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"oCajWknYcYpV"},"source":["The angle rates range from `1 [rads/step]` to `min_rate [rads/step]` over the vector depth.\n","\n","Formula for angle rate:\n","\n","$$angle\\_rate_d = (min\\_rate)^{d / d_{max}} $$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ADsz4y85ANsB"},"outputs":[],"source":["num_positions = 50\n","# depth = 512\n","depth = 100\n","\n","min_rate = 1/10000\n","\n","assert depth%2 == 0, \"Depth must be even.\"\n","angle_rate_exponents = np.linspace(0,1,depth//2)\n","# print(angle_rate_exponents, depth//2)\n","angle_rates = min_rate**(angle_rate_exponents)\n","# print(angle_rates)\n"]},{"cell_type":"markdown","metadata":{"id":"E51Ne4cNe1i8"},"source":["The resulting exponent goes from `0` to `1`, causing the `angle_rates` to drop exponentially from `1` to `min_rate`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ohldgW_Xg2sC"},"outputs":[],"source":["plt.semilogy(angle_rates)\n","plt.xlabel('Depth')\n","plt.ylabel('Angle rate [rads/step]')"]},{"cell_type":"markdown","metadata":{"id":"fMaA6pUqh-tQ"},"source":["Broadcasting a multiply over angle rates and positions gives a map of the position encoding angles as a function of depth."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rs2ABZ3XhYFg"},"outputs":[],"source":["positions = np.arange(num_positions)\n","print(positions)\n","angle_rads = (positions[:, np.newaxis])*angle_rates[np.newaxis, :]\n","print(angle_rads, angle_rads.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eHEHWQa8_51Z"},"outputs":[],"source":["plt.figure(figsize = (14,8))\n","plt.pcolormesh(\n","    # Convert to degrees, and wrap around at 360\n","    angle_rads*180/(2*np.pi) % 360,\n","    # Use a cyclical colormap so that color(0) == color(360)\n","    cmap='hsv', vmin=0, vmax=360)\n","\n","plt.xlim([0,len(angle_rates)])\n","plt.ylabel('Position')\n","plt.xlabel('Depth')\n","bar = plt.colorbar(label='Angle [deg]')\n","bar.set_ticks(np.linspace(0,360,6+1))"]},{"cell_type":"markdown","metadata":{"id":"MEGyhu_JiNDS"},"source":["Raw angles are not a good model input (they're either unbounded, or discontinuous). So take the sine and cosine:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBnQuSSPGl8t"},"outputs":[],"source":["sines = np.sin(angle_rads)\n","cosines = np.cos(angle_rads)\n","\n","# print(sines)\n","# print(cosines)\n","\n","\n","\n","\n","# pos_encoding = np.concatenate([sines, cosines], axis=-1)\n","\n","# print(pos_encoding)\n","\n","# print(pos_encoding.shape)\n","\n","# pos_encoding = sines\n","\n","# n 2n+1\n","pos_encoding = np.stack((sines, cosines), axis=-1)\n","\n","print(pos_encoding.shape)\n","\n","pos_encoding = np.reshape(pos_encoding, (num_positions, depth))\n","print(pos_encoding.shape)\n","\n"]},{"cell_type":"code","source":["# xs = np.random.rand(50, 100)\n","xs = np.ones((50, 100))\n","\n","pos_encoding = xs + pos_encoding"],"metadata":{"id":"E3VxxDWke-RI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQrE2_nPHjXC"},"outputs":[],"source":["plt.figure(figsize=(14,8))\n","plt.pcolormesh(pos_encoding,\n","               # Use a diverging colormap so it's clear where zero is.\n","               cmap='RdBu', vmin=-1, vmax=1)\n","plt.xlim([0,depth])\n","plt.ylabel('Position')\n","plt.xlabel('Depth')\n","plt.colorbar()"]},{"cell_type":"markdown","source":["## Similarity"],"metadata":{"id":"5iuevffGluip"}},{"cell_type":"code","source":["def cosine_similarity(vec1, vec2):\n","    vec1_norm = np.linalg.norm(vec1) + 0.00000000001\n","    vec2_norm = np.linalg.norm(vec2) + 0.00000000001\n","    return np.dot(vec1, vec2) / (vec1_norm * vec2_norm)\n","\n","def cosine_similarity_matrix(vectors):\n","    return np.array([[cosine_similarity(vec1, vec2) for vec2 in vectors] for vec1 in vectors])\n","\n","similarity_matrix = cosine_similarity_matrix(pos_encoding)\n","print(similarity_matrix,similarity_matrix.shape)\n","print(np.ptp(similarity_matrix, axis=1))\n","max_element = np.max(similarity_matrix)\n","min_element = np.min(similarity_matrix)\n","print(max_element, min_element)\n","\n","\n","def norms(vectors):\n","    return np.array([np.linalg.norm(vec1) for vec1 in vectors])\n","\n","def means(vectors):\n","    return np.array([np.mean(vec1) for vec1 in vectors])\n","\n","print('norms', norms(pos_encoding))\n","print('means', means(pos_encoding))\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"GR29gz3vl4eM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8,8))\n","plt.pcolormesh(similarity_matrix,\n","               # Use a diverging colormap so it's clear where zero is.\n","               cmap='RdBu', vmin=-1, vmax=1)\n","plt.xlim([0,num_positions])\n","plt.ylabel('1-50')\n","plt.xlabel('1-50')\n","plt.colorbar()"],"metadata":{"id":"WFepRlT3mVbU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e536lUb-k-UQ"},"source":["## Nearby positions"]},{"cell_type":"markdown","metadata":{"id":"UzWWHKK1lu0z"},"source":["Nearby locations will have similar position-encoding vectors.\n","\n","To demonstrate compare one position's encoding (here position 20) with each of the others:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kruiT670zjrZ"},"outputs":[],"source":["pos_encoding_at_20 = pos_encoding[20]\n","\n","dots = np.dot(pos_encoding,pos_encoding_at_20)\n","SSE = np.sum((pos_encoding - pos_encoding_at_20)**2, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"xH-sU6mzmjXI"},"source":["Regardless of how you compare the vectors, they are most similar 20, and clearly diverge as you move away:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQ-MqQ5Eo0ZT"},"outputs":[],"source":["plt.figure(figsize=(10,8))\n","plt.subplot(2,1,1)\n","plt.plot(dots)\n","plt.ylabel('Dot product')\n","plt.subplot(2,1,2)\n","plt.plot(SSE)\n","plt.ylabel('SSE')\n","plt.xlabel('Position')\n"]},{"cell_type":"markdown","metadata":{"id":"BVgAzBJ7iYda"},"source":["## Relative positions\n","\n","The [paper](https://arxiv.org/pdf/1706.03762.pdf) explains, at the end of section 3.5, that any relative position encoding can be written as a linear function of the current position.\n","\n","To demonstrate, this section builds a matrix that calculates these relative position encodings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aHOBuNYnLHFF"},"outputs":[],"source":["def transition_matrix(position_delta, angle_rates = angle_rates):\n","  # Implement as a matrix multiply:\n","  #    sin(a+b) = sin(a)*cos(b)+cos(a)*sin(b)\n","  #    cos(a+b) = cos(a)*cos(b)-sin(a)*sin(b)\n","\n","  # b\n","  angle_delta = position_delta*angle_rates\n","\n","  # sin(b), cos(b)\n","  sin_delta = np.sin(angle_delta)\n","  cos_delta = np.cos(angle_delta)\n","\n","  I = np.eye(len(angle_rates))\n","\n","  # sin(a+b) = sin(a)*cos(b)+cos(a)*sin(b)\n","  update_sin = np.concatenate([I*cos_delta, I*sin_delta], axis=0)\n","\n","  # cos(a+b) = cos(a)*cos(b)-sin(a)*sin(b)\n","  update_cos = np.concatenate([-I*sin_delta, I*cos_delta], axis=0)\n","\n","  return np.concatenate([update_sin, update_cos], axis=-1)"]},{"cell_type":"markdown","metadata":{"id":"GZKHXFAxkMb_"},"source":["For example, create the matrix that calculates the position encoding 10 steps back, from the current position encoding:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x31yfQRQM6Q6"},"outputs":[],"source":["position_delta = -10\n","update = transition_matrix(position_delta)"]},{"cell_type":"markdown","metadata":{"id":"NznTFtWmklob"},"source":["Applying this matrix to each position encoding vector gives position encoding vector from -10 steps away, resulting in a shifted position-encoding map:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hE7t9frqNQV3"},"outputs":[],"source":["plt.figure(figsize=(14,8))\n","plt.pcolormesh(np.dot(pos_encoding,update), cmap='RdBu', vmin=-1, vmax=1)\n","plt.xlim([0,depth])\n","plt.ylabel('Position')\n","plt.xlabel('Depth')"]},{"cell_type":"markdown","metadata":{"id":"a1IHJT2Jk16a"},"source":["This is accurate to numerical precision."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tRmt2d-HQD8j"},"outputs":[],"source":["errors = np.dot(pos_encoding,update)[10:] - pos_encoding[:-10]\n","abs(errors).max()"]}],"metadata":{"accelerator":"GPU","colab":{"private_outputs":true,"provenance":[{"file_id":"https://github.com/tensorflow/examples/blob/679fff86d42ba5854ca8378ce7ec16c720580c26/community/en/position_encoding.ipynb","timestamp":1692169642574}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}