{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1VM0uolTFFxosakT5ox1Y9TUTjVVKLKzP","timestamp":1678422524548}],"private_outputs":true,"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"262.5px"},"toc_section_display":true,"toc_window_display":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"DrC4wpoZBfEj"},"source":["This notebook shows how to export into the SavedModel format a custom training loop and for later training in a different environment"]},{"cell_type":"code","metadata":{"id":"lXsXev5MNr20"},"source":["!pip install tensorflow==2.3.0\n","import os\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import tensorflow as tf\n","import tensorflow.keras as keras"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6GM1GRTHBfEz"},"source":["# Training Step in tf.function"]},{"cell_type":"code","metadata":{"id":"EKgrWJ4BBfE1"},"source":["np.random.seed(2); tf.random.set_seed(5)\n","\n","def make_model():\n","    # this constructs a keras Model. We use the functional API and add a custom\n","    # layer for demo purposes but a model of any complexity can be used here\n","    from tensorflow.keras import layers\n","    \n","    class CustomLayer(keras.layers.Layer):\n","        def __init__(self, **kwargs):\n","            super().__init__(**kwargs)\n","            l2_reg = keras.regularizers.l2(0.1)\n","            self.dense = layers.Dense(1, kernel_regularizer=l2_reg, \n","                                      name='my_layer_dense')\n","            \n","        def call(self, data):\n","            return self.dense(data)\n","    inputs = keras.Input(shape=(8,))\n","    x1 = layers.Dense(30, activation=\"relu\", name='my_dense')(inputs)\n","    outputs = CustomLayer()(x1)\n","    return keras.Model(inputs=inputs, outputs=outputs)\n","\n","# Prepare the training dataset.\n","def get_housing_dataset():\n","    from sklearn.datasets import fetch_california_housing\n","    from sklearn.model_selection import train_test_split\n","    from sklearn.preprocessing import StandardScaler\n","    housing = fetch_california_housing()\n","\n","    X_train_full, X_test, y_train_full, y_test = train_test_split(\n","        housing.data, housing.target)\n","    X_train, X_valid, y_train, y_valid = train_test_split(\n","        X_train_full, y_train_full)\n","\n","    scaler = StandardScaler()\n","    X_train = scaler.fit_transform(X_train).astype(np.float32)\n","    X_valid = scaler.transform(X_valid).astype(np.float32)\n","    X_test = scaler.transform(X_test).astype(np.float32)\n","    return X_train, X_valid, X_test, y_train.astype(np.float32), \\\n","           y_valid.astype(np.float32), y_test.astype(np.float32)\n","\n","X_train, X_valid,_, y_train, y_valid, _ = get_housing_dataset()\n","\n","batch_size = 64\n","train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n","train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n","valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(batch_size)\n","\n","\n","class CustomModule(tf.Module):\n","\n","    def __init__(self):\n","        super(CustomModule, self).__init__()\n","        self.model = make_model()\n","        self.opt = keras.optimizers.Adam(learning_rate=0.001)\n","        \n","    # add @tf.function here to make it faster (run in graph mode) and ensure the right shapes and types\n","    # are used (optional). \n","    # To debug we can \n","    # - either use tf.print() statements that will execute in graph mode\n","    # - or run in eager mode by removing the @tf.function annotation or by specifying\n","    #   tf.config.experimental_run_functions_eagerly(True). In eager mode print() or any python \n","    #   statement can be used (instead of tf.print()) and we can use debugger breakpoint\n","    \n","    @tf.function(input_signature=[tf.TensorSpec([None, 8], tf.float32)])\n","    def __call__(self, X):\n","        return self.model(X)\n","\n","    # the my_train function processes one batch (one step): computes the loss and apply the\n","    # loss gradient to update the model weights\n","    @tf.function(input_signature=[tf.TensorSpec([None, 8], tf.float32), tf.TensorSpec([None], tf.float32)])\n","    def my_train(self, X, y):\n","        with tf.GradientTape() as tape:\n","            logits = self.model(X, training=True)  \n","            main_loss = tf.reduce_mean(keras.losses.mean_squared_error(y, logits))\n","            # self.model.losses contains the reularization loss (see l2_reg above)\n","            loss_value = tf.add_n([main_loss] + self.model.losses) \n","\n","        grads = tape.gradient(loss_value, self.model.trainable_weights)\n","        self.opt.apply_gradients(zip(grads, self.model.trainable_weights))\n","        return loss_value\n","\n","# set to True to force in eager execution despite @tf.functions (debugging)\n","tf.config.run_functions_eagerly(False)\n","\n","# instantiate the module\n","module = CustomModule()\n","\n","# demo a call to the module. (calls the __call__() method)\n","print('sample prediction: ', module(X_train[0:1]).numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WOxAJG4eBfE-"},"source":["Let's call the `my_train` function repeatedly to train the model"]},{"cell_type":"code","metadata":{"id":"uWgWR2vLBfFA"},"source":["def train_module(module, train_dataset, valid_dataset):\n","    valid_metric = keras.metrics.MeanSquaredError()\n","    loss_hist = []\n","    step=1\n","    for epoch in range(3):\n","        for X, y in train_dataset:\n","            loss = module.my_train(X, y)\n","            loss_hist.append(loss.numpy())\n","        \n","            if step % 100 == 0:\n","                for (X_val, y_val) in valid_dataset:\n","                    val_logits = module(X_val)\n","                    valid_metric.update_state(y_val, val_logits)\n","                print(f'Mean squared error: step {step}: {valid_metric.result()}')\n","            step+=1\n","    return loss_hist    \n","\n","def plot_loss(loss_hist):\n","    plt.figure(figsize=(8,4))\n","    plt.title('loss', fontsize=15)\n","    plt.plot(loss_hist)\n","    plt.grid()\n","    \n","# train the module\n","loss_hist = train_module(module, train_dataset, valid_dataset)\n","plot_loss(loss_hist)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_W9pdH4xBfFF"},"source":["Let's check the state of the `ADAM` optimizer that we use. ADAM learns two variables `m` and `v` associated to each weight.\n","\n","`m` and `v` are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively  \n","For more info: https://ruder.io/optimizing-gradient-descent/index.html#adam\n","\n","In Tensorflow terms they are called `slots`. For more info about how they are tracked see https://www.tensorflow.org/guide/checkpoint#loading_mechanics\n","\n","Let's examine the content of ADAM's m slot for the bias of the first dense layer"]},{"cell_type":"code","metadata":{"id":"WimNxpNYBfFH"},"source":["module.opt.weights[2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FpDUQ4HFBfFQ"},"source":["Let's train a bit further. As expected the loss remains low (note the scale of this plot)"]},{"cell_type":"code","metadata":{"id":"5QveEcoBBfFS"},"source":["loss_hist = train_module(module, train_dataset, valid_dataset)\n","plot_loss(loss_hist)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gHEurz6FBfFb"},"source":["We can check then that the variables of ADAM optimizer changed as well: "]},{"cell_type":"code","metadata":{"id":"FI0g1X3-BfFc"},"source":["module.opt.weights[2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qHm9vePMBfFi"},"source":["# Persist Model and continue Training\n","\n","All the above was done in memory. Similarly when we save a model it is nice that not only the weights of the layers are saved but also the optimizer state. Then when we continue training a reloaded a model the optimizer doesn't need to re-learn its variables (the `m` and `v` moments in the case of ADAM).\n","\n","Let's save the module in the SavedModel format. The SavedModel format contains signatures that describe the exported functions with their inputs and outputs available when we load the model. For more info see https://www.tensorflow.org/guide/saved_model\n","\n","Then we'll inspect the content of the checkpoint saved with the model"]},{"cell_type":"code","metadata":{"id":"2878hGMFBfFi"},"source":["def save_module(module, model_dir):\n","    \n","    # When saving a tf.keras.Model with either model.save() or \n","    # tf.keras.models.save_model() or tf.saved_model.save(),\n","    # the saved model contains a `serving_default` signature used to get the \n","    # output of the model from an input sample. But here we don't save a keras \n","    # Model but a tf.Module. This requires to specify the signatures manually\n","    \n","    # Note that we also export the training function here\n","    \n","    tf.saved_model.save(module, model_dir, \n","        signatures={\n","            'my_serve' : \n","            module.__call__.get_concrete_function(tf.TensorSpec([None, 8], tf.float32)),\n","            'my_train' : \n","            module.my_train.get_concrete_function(tf.TensorSpec([None, 8], tf.float32), \n","                                                  tf.TensorSpec([None], tf.float32))})\n","    \n","def inspect_checkpoint(checkpoint, print_values=False, variables=None):\n","    if not variables:\n","        variables = [var_name for (var_name, shape) in tf.train.list_variables(checkpoint)]\n","\n","    checkpoint_reader = tf.train.load_checkpoint(checkpoint)\n","    for var_name in variables:\n","            \n","        try:\n","            tensor = checkpoint_reader.get_tensor(var_name)\n","        except Exception as e:\n","            print('ignored   : %s (exception %s)' % (var_name, str(type(e))))\n","            continue\n","        if isinstance(tensor, np.ndarray):\n","            if print_values:\n","                print('tensor    : ', var_name, tensor.shape, tensor)\n","            else:\n","                print('tensor    : ', var_name, tensor.shape)\n","        else:\n","            if print_values:\n","                print('non-tensor: ', var_name, type(tensor), tensor)\n","            else:\n","                print('non-tensor: ', var_name, type(tensor))\n","\n","model_dir = 'saved_model'\n","os.makedirs(model_dir, exist_ok=True)\n","\n","save_module(module, model_dir)\n","         \n","inspect_checkpoint(model_dir + '/variables/variables')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cz7iPuG4BfFm"},"source":["We can see not only the layers weights but also the ADAM's slot variables (`m` and `v`) discussed earlier.  \n","\n","Excerpt from https://www.tensorflow.org/guide/saved_model#saving_a_custom_model\n","\n","> When you save a `tf.Module`, any `tf.Variable` attributes, `tf.function`-decorated methods, and `tf.Modules` found via recursive traversal are saved. (See the [Checkpoint tutorial](https://www.tensorflow.org/guide/checkpoint) for more about this recursive traversal.)\n","\n","\n","This is covered in great lengths in https://www.tensorflow.org/guide/checkpoint#loading_mechanics  \n","\n","Let's examine the content of ADAM's m slot for the bias of the first dense layer"]},{"cell_type":"code","metadata":{"id":"smtaOhleBfFn"},"source":["inspect_checkpoint(model_dir + '/variables/variables', print_values=True, \n","                   variables=['model/layer_with_weights-0/bias/.OPTIMIZER_SLOT/opt/m/.ATTRIBUTES/VARIABLE_VALUE'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1c_7r5RRBfFw"},"source":["We can see that this is exactly the in-memory content of that slot checked above with\n","```python\n","module.opt.weights[2]\n","``` \n","This shows that the ADAM's state was indeed saved in the checkpoint. Let's have a look at the exported signatures with the `saved_model_cli` tool bundled with Tensorflow"]},{"cell_type":"code","metadata":{"id":"f5GPAYcxBfFy"},"source":["!saved_model_cli show --all --dir $model_dir"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aHh9_76kBfF2"},"source":["We see the expected signatures for the prediction (`my_serve`) and training (`my_training`) exported functions. More on this later.   \n","Let's create a fresh instance of the module, save it untrained and reload it "]},{"cell_type":"code","metadata":{"id":"xETjiejtBfF2"},"source":["# instantiate a new module and save it untrained\n","module = CustomModule()\n","save_module(module, model_dir)\n","\n","del module\n","\n","print('\\n\\n========== Reload module ===========')\n","\n","# the following works also if we reload in another python process\n","model_dir = 'saved_model'\n","new_module = tf.keras.models.load_model(model_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"irFw8p6CBfF7"},"source":["What's noteworthy here is that the loaded `new_module` object is not a `tf.Module` instance but another kind of object that still offers our `my_train` and `__call__`  functions that we exported. Let's call the `__call__()` method to see that it works (will yield anything since the model is not yet trained)"]},{"cell_type":"code","metadata":{"id":"BYu4wFqPBfF9"},"source":["print('type of reloaded module:', type(new_module))\n","print('type of instantiated module:', type(CustomModule()))\n","print('my_train function:', new_module.my_train)\n","print('__call__ function:', new_module.__call__)\n","\n","# demo a call to the module. (calls the __call__() method)\n","print('sample prediction: ', new_module(X_train[0:1]).numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MGtcWEPaBfGE"},"source":["As a consequence we can still use the `my_train` function with our `train_module` function. Let's train the reloaded module and save it afterwards"]},{"cell_type":"code","metadata":{"id":"SOOv34cmBfGF"},"source":["np.random.seed(3); tf.random.set_seed(5)\n","loss_hist = train_module(new_module, train_dataset, valid_dataset)\n","plot_loss(loss_hist)\n","\n","save_module(new_module, model_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zRgzhPpHBfGJ"},"source":["The above shows that we can load a module and train it exactly as if we had instantiated it with `CustomModule()`. Let's check some of the optimizer state like we did above"]},{"cell_type":"code","metadata":{"id":"ySn6cHzYBfGJ"},"source":["inspect_checkpoint(model_dir + '/variables/variables', print_values=True, \n","                   variables=['model/layer_with_weights-0/bias/.OPTIMIZER_SLOT/opt/m/.ATTRIBUTES/VARIABLE_VALUE'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0D3PtyQaBfGS"},"source":["Reload the module, continue the training and save it"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"IL6Fz5tRBfGS"},"source":["del new_module\n","new_module_2 = tf.keras.models.load_model(model_dir)\n","\n","loss_hist = train_module(new_module_2, train_dataset, valid_dataset)\n","plot_loss(loss_hist)\n","\n","save_module(new_module_2, model_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3kV_ptkJBfGV"},"source":["Like what we did in memory-only earlier the above shows that the weights have been correctly reloaded and that we didn't restart the training from scratch. How about the ADAM's variables ?"]},{"cell_type":"code","metadata":{"id":"GDglEjPIBfGW"},"source":["inspect_checkpoint(model_dir + '/variables/variables', print_values=True, \n","                   variables=['model/layer_with_weights-0/bias/.OPTIMIZER_SLOT/opt/m/.ATTRIBUTES/VARIABLE_VALUE'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mZpoY0HsBfGa"},"source":["The ADAM's variables have changed as well. They change less and less as training continues which can be shown by taking the norm of the difference of those slots before and after training and seeing that this norm decreases with time. \n","\n","This shows that the optimizer state is also captured in the saved model and that we can stop and resume training without losing anything. "]},{"cell_type":"markdown","metadata":{"id":"gKnYgNpYBfGb"},"source":["# Low Level Operations\n","All the above was done with python objects and methods available when reloading a module. But how can we do it in another language where only the graph and operations are available ? \n","\n","Let's see first how to do it in python. Here's again the `saved_model_cli` ouput for the `my_train` signature:\n","```\n","signature_def['my_train']:\n","  The given SavedModel SignatureDef contains the following input(s):\n","    inputs['X'] tensor_info:\n","        dtype: DT_FLOAT\n","        shape: (-1, 8)\n","        name: my_train_X:0\n","    inputs['y'] tensor_info:\n","        dtype: DT_FLOAT\n","        shape: (-1)\n","        name: my_train_y:0\n","  The given SavedModel SignatureDef contains the following output(s):\n","    outputs['output_0'] tensor_info:\n","        dtype: DT_FLOAT\n","        shape: ()\n","        name: StatefulPartitionedCall_1:0\n","  Method name is: tensorflow/serving/predict\n"," ```\n"," \n","It turns out we can access the input and output tensors by the names shown here. For example:\n"," - `inputs['X']` has name `my_train_x:0`\n"," - `output['output_0']` (the loss) has name `StatefulPartitionedCall_1:0`  \n","\n","Something that is hidden is the operation and tensor used to save the model\n","- name of the checkpoint: `saver_filename:0` : must point to `model_dir + '/variables/variables'`\n","- save operation: `StatefulPartitionedCall_2:0`: the next `StatefulPartitionedCall` after the ones exported by our module\n","\n","The information about the save operation is of course not documented, probably intentionally, so this might not work in future tensorflow versions."]},{"cell_type":"code","metadata":{"id":"3ktyjgL5BfGc"},"source":["def train_predict_serve(model_dir):\n","    tf.compat.v1.reset_default_graph()\n","    session = tf.compat.v1.Session()\n","    tf.compat.v1.saved_model.loader.load(session, tags=[tf.saved_model.SERVING], export_dir=model_dir)\n","    graph = session.graph\n","    operations=graph.get_operations()\n","    \n","    input_X = graph.get_tensor_by_name('my_train_X:0')\n","    input_y = graph.get_tensor_by_name('my_train_y:0')\n","    output_loss = graph.get_tensor_by_name('StatefulPartitionedCall_1:0')\n","    \n","    loss = session.run(output_loss, feed_dict={input_X: X_train[0:batch_size], \n","                                               input_y: y_train[0:batch_size]})\n","    print('loss:', loss)\n","    \n","    input_X_serve = graph.get_tensor_by_name('my_serve_X:0')\n","    output_pred = graph.get_tensor_by_name('StatefulPartitionedCall:0')\n","    \n","    pred = session.run(output_pred, feed_dict={input_X_serve: X_train[0:1]})\n","    print('prediction:', pred)\n","    \n","    saver_filename = graph.get_tensor_by_name('saver_filename:0')\n","    save_op = graph.get_tensor_by_name('StatefulPartitionedCall_2:0')\n","    session.run(save_op, feed_dict={saver_filename: model_dir + '/variables/variables'})\n","    print('checkpoint saved')\n","    \n","    session.close()\n","    \n","train_predict_serve(model_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KmjKuVusBfGf"},"source":["If you execute `train_predict_serve()` more than once, you'll get different results since the model is training and predictions change.\n","\n","The above shows that we can train, save a module and make predictions with only low level operations.  \n","This allows to **export blank models** with their training and serve graphs and let a 3rd party organization train it and make predictions. The exported operations are enough to let that organization train and monitor loss decrease, report accuracy on validation data sets and make inference."]},{"cell_type":"markdown","metadata":{"id":"7fTvIs2oBfGh"},"source":["*Side note*: if the module function returned two outputs, `saved_model_cli` would report them this way:\n","```\n","  The given SavedModel SignatureDef contains the following output(s):\n","    outputs['output_0'] tensor_info:\n","        dtype: DT_FLOAT\n","        shape: ()\n","        name: StatefulPartitionedCall_1:0\n","    outputs['output_1'] tensor_info:\n","        dtype: DT_FLOAT\n","        shape: ()\n","        name: StatefulPartitionedCall_1:1\n","```\n","And they could be fetched this way:  \n","```python\n","    input_X = graph.get_tensor_by_name('my_train_X:0')\n","    input_y = graph.get_tensor_by_name('my_train_y:0')\n","    output_1 = graph.get_tensor_by_name('StatefulPartitionedCall_1:0')\n","    output_2 = graph.get_tensor_by_name('StatefulPartitionedCall_1:1')\n","    \n","    out_val_1, out_val_2 = session.run([output_1, output_2], \n","                                       feed_dict={input_X: X_train[0:1], input_y: y_train[0:1]})\n","```"]},{"cell_type":"markdown","metadata":{"id":"yZOXGLlzBfGh"},"source":["The same exported model can be used for training and prediction in java with this code:\n","\n","```java\n","public class TrainAndServeSavedModel {\n","    public static void main(String[] args) throws Exception {\n","\n","        // args[0]: saved model directory\n","        SavedModelBundle savedModel = SavedModelBundle.load(args[0], \"serve\");\n","        Map<String, SignatureDef> signatureMap = savedModel.metaGraphDef().getSignatureDefMap();\n","\n","        Tensor<TFloat32> inputTensor = TFloat32.tensorOf(StdArrays.ndCopyOf(new float[][] { { 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f } }));\n","        Tensor<TFloat32> labelTensor = TFloat32.tensorOf(StdArrays.ndCopyOf(new float[] { 1.0f }));\n","        \n","        Session session = savedModel.session();\n","        train(session, signatureMap.get(\"my_train\"), inputTensor, labelTensor);\n","        serve(session, signatureMap.get(\"my_serve\"),  inputTensor);\n","        session.close();\n","    }\n","\n","    private static void serve(Session session, SignatureDef modelInfo, Tensor<TFloat32> inputTensor) {\n","        Map<String, TensorInfo> inputs = modelInfo.getInputsMap();\n","        TensorInfo inputX = inputs.get(\"x\");\n","        TensorInfo outputPred = modelInfo.getOutputsMap().get(\"output_0\");\n","\n","        Session.Runner runner = session.runner();\n","        runner.feed(inputX.getName(), inputTensor);\n","        TFloat32 data = runner.fetch(outputPred.getName()).run().get(0).expect(TFloat32.DTYPE).data();\n","        data.scalars().forEachIndexed((i, s) -> {\n","            System.out.println(\"prediction: \" + s.getFloat());\n","        });\n","    }\n","\n","    private static void train(Session session, SignatureDef modelInfo, Tensor<TFloat32> inputTensor, Tensor<TFloat32> labelTensor) {\n","        Map<String, TensorInfo> inputs = modelInfo.getInputsMap();\n","        TensorInfo inputX = inputs.get(\"X\");\n","        TensorInfo inputY = inputs.get(\"y\");\n","        TensorInfo outputLoss = modelInfo.getOutputsMap().get(\"output_0\");\n","\n","        Session.Runner runner = session.runner();\n","        runner.feed(inputX.getName(), inputTensor).feed(inputY.getName(), labelTensor);\n","        Tensor<TFloat32> loss = runner.fetch(outputLoss.getName()).run().get(0).expect(TFloat32.DTYPE);\n","        System.out.println(\"loss after training: \" + loss.data().getFloat());\n","    }\n","}\n","```\n","\n","Prints this:\n","\n","```\n","loss after training: 1.2554951\n","prediction: 2.1101139\n","```\n","\n","Tested with tensorflow 2.3.0. For more info:  \n","https://github.com/tensorflow/java  \n","https://www.tensorflow.org/api_docs/java/reference/org/tensorflow/package-summary  \n"]},{"cell_type":"markdown","metadata":{"id":"k42ZO38JBfGj"},"source":["# Conclusion\n","\n","We've shown how to export a training step into the SavedModel format and how to invoke it on a reloaded model in python as well as with low level operations in python or another language like java.\n","\n","Other useful link about SavedModel manipulation (thanks Drew Hodun)  \n","https://towardsdatascience.com/how-to-extend-a-keras-model-5effc083265c"]}]}